{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de imágenes con PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se parte del mismo modelo de Fashion MNIST, pero cambiando el dataset por MNIST. Además fijamos la semilla para que los resultados sean reproducibles.\n",
    "\n",
    "<!-- TODO: https://www.reddit.com/r/learnmachinelearning/comments/11m0tyy/what_exactly_is_happening_when_i_dont_normalize/ -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "_ = torch.manual_seed(42) # Fijamos la semilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos como tensores, pero esta vez calcularemos su media y desviación estándar para normalizarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.13066041469573975, Std: 0.30150410532951355\n"
     ]
    }
   ],
   "source": [
    "def calculate_mean_std(dataloader):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    total_samples = 0.\n",
    "    for data, _ in dataloader:\n",
    "        batch_samples = data.size(0)\n",
    "        data = data.view(batch_samples, data.size(1), -1)\n",
    "        mean += data.mean(2).sum(0)\n",
    "        std += data.std(2).sum(0)\n",
    "        total_samples += batch_samples\n",
    "\n",
    "    mean /= total_samples\n",
    "    std /= total_samples\n",
    "    return mean, std\n",
    "\n",
    "mean, std = calculate_mean_std(dataloader)\n",
    "print(f\"Mean: {mean.item()}, Std: {std.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a cargar los datos aplicando esta vez la transformación de normalización.\n",
    "\n",
    "(Sin embargo, ya que luego se aplicará normalización por lotes, finalmente he dejado esta transformación comentada. De hecho los resultados son mejores sin hacerla)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                          #    transforms.Normalize((mean,), (std,)),\n",
    "                                ])\n",
    "\n",
    "training_data = datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root=\"data\", train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso utilizo un único *batch* para todo el grupo de test, ya que no es necesario hacerlo por partes (todos los datos caben sobradamente en memoria) y reducirá el tiempo de ejecución. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FNN(nn.Module): \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),   \n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.2),      \n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = FNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se han añadido capas de normalización de tipo *BatchNorm2d* (normalización por *batch* en 2 dimensiones) y *Dropout* (para evitar el sobreajuste).\n",
    "\n",
    "BatchNorm2d normaliza la entrada por *batch* de tal forma que la media y la desviación estándar sean 0 y 1 respectivamente. Dropout desactiva aleatoriamente un porcentaje de las neuronas de la capa anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available() \n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\" \n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimización y entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ha utilizado el optimizador Adam, siendo el cambio más significativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model : nn.Module, loss_fn, optimizer):    \n",
    "    model.train()\n",
    "    for X, y in dataloader:\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: \n",
      "Accuracy: 97.0%, Avg loss: 0.097208\n",
      "\n",
      "Epoch 2: \n",
      "Accuracy: 97.4%, Avg loss: 0.078898\n",
      "\n",
      "Epoch 3: \n",
      "Accuracy: 97.8%, Avg loss: 0.071459\n",
      "\n",
      "Epoch 4: \n",
      "Accuracy: 98.0%, Avg loss: 0.067445\n",
      "\n",
      "Epoch 5: \n",
      "Accuracy: 97.7%, Avg loss: 0.079866\n",
      "\n",
      "Epoch 6: \n",
      "Accuracy: 98.3%, Avg loss: 0.056848\n",
      "\n",
      "Epoch 7: \n",
      "Accuracy: 98.2%, Avg loss: 0.060812\n",
      "\n",
      "Epoch 8: \n",
      "Accuracy: 98.2%, Avg loss: 0.061460\n",
      "\n",
      "Epoch 9: \n",
      "Accuracy: 98.2%, Avg loss: 0.063102\n",
      "\n",
      "Epoch 10: \n",
      "Accuracy: 98.2%, Avg loss: 0.067923\n",
      "\n",
      "Epoch 11: \n",
      "Accuracy: 98.2%, Avg loss: 0.077423\n",
      "\n",
      "Epoch 12: \n",
      "Accuracy: 98.4%, Avg loss: 0.062102\n",
      "\n",
      "Epoch 13: \n",
      "Accuracy: 98.5%, Avg loss: 0.082789\n",
      "\n",
      "Epoch 14: \n",
      "Accuracy: 98.3%, Avg loss: 0.068258\n",
      "\n",
      "Epoch 15: \n",
      "Accuracy: 98.0%, Avg loss: 0.104543\n",
      "\n",
      "Epoch 16: \n",
      "Accuracy: 98.2%, Avg loss: 0.079497\n",
      "\n",
      "Epoch 17: \n",
      "Accuracy: 98.5%, Avg loss: 0.067877\n",
      "\n",
      "Epoch 18: \n",
      "Accuracy: 98.4%, Avg loss: 0.075240\n",
      "\n",
      "Epoch 19: \n",
      "Accuracy: 98.5%, Avg loss: 0.065426\n",
      "\n",
      "Epoch 20: \n",
      "Accuracy: 98.4%, Avg loss: 0.070814\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    print(f\"Epoch {t+1}: \")\n",
    "    test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuentes\n",
    "- https://stackoverflow.com/questions/63746182/correct-way-of-normalizing-and-scaling-the-mnist-dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
