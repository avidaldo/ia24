{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimización de hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pasos previos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición del *pipeline*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipeline = make_pipeline( # Pipeline for categorical features\n",
    "    SimpleImputer(strategy=\"most_frequent\"), # Impute missing values with the most frequent value\n",
    "    OneHotEncoder(handle_unknown=\"ignore\")) # One-hot encode the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterSimilarity(BaseEstimator, TransformerMixin): # Custom transformer to compute similarity with cluster center\n",
    "    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.gamma = gamma # RBF kernel bandwidth\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "        self.kmeans_ = KMeans(self.n_clusters, n_init=10, \n",
    "                              random_state=self.random_state)\n",
    "        self.kmeans_.fit(X, sample_weight=sample_weight)\n",
    "        return self  # always return self!\n",
    "\n",
    "    def transform(self, X):\n",
    "        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\n",
    "    \n",
    "    def get_feature_names_out(self, names=None):\n",
    "        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)]\n",
    "\n",
    "cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_ratio(X): # Custom transformer to compute the ratio of two columns\n",
    "    return X[:, [0]] / X[:, [1]]\n",
    "\n",
    "def ratio_name(function_transformer, feature_names_in): # Custom function to name the output columns\n",
    "    return [\"ratio\"]  # feature names out\n",
    "\n",
    "def ratio_pipeline(): # Pipeline for ratio features (create new features by dividing two columns)\n",
    "    return make_pipeline(\n",
    "        SimpleImputer(strategy=\"median\"),\n",
    "        FunctionTransformer(column_ratio, feature_names_out=ratio_name),\n",
    "        StandardScaler())\n",
    "\n",
    "log_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\n",
    "    StandardScaler())\n",
    "\n",
    "default_num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"),\n",
    "                                     StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = ColumnTransformer([\n",
    "        (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]), # razón entre total_bedrooms y total_rooms (nueva feature)\n",
    "        (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]), # razón entre total_rooms y households (nueva feature)\n",
    "        (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]), # razón entre population y households (nueva feature)\n",
    "        (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\",\n",
    "                               \"households\", \"median_income\"]), # logaritmo de las columnas seleccionadas (para cambiar distribuciones sesgadas -skewed- por distribuciones normales)\n",
    "        (\"geo\", cluster_simil, [\"latitude\", \"longitude\"]), # similitud con los clusters\n",
    "        (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)), # pipeline categórico\n",
    "    ],\n",
    "    remainder=default_num_pipeline)  # one column remaining: housing_median_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = Pipeline([\n",
    "    (\"preprocessing\", preprocessing),\n",
    "    (\"random_forest\", RandomForestRegressor(random_state=42, n_jobs=1)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación y preparación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv(\"./data/housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "strat_train_set, strat_test_set = train_test_split(\n",
    "    housing, test_size=0.2, stratify=housing[\"income_cat\"], random_state=42)\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)\n",
    "    \n",
    "X_train = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "y_train = strat_train_set[\"median_house_value\"].copy()\n",
    "\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparámetros relevantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el pipeline de preprocesamiento:\n",
    "\n",
    "| Hiperparámetro      | Descripción                                                 |\n",
    "|---------------------|-------------------------------------------------------------|\n",
    "| `n_clusters`        | Números de clusters correspondientes a zonas geográficas.   |\n",
    "| `gamma`             | Velocidad de caída de la similitud con el centroide.        |\n",
    "| `strategy`          | Estrategia de imputación de valores no disponibles (por defecto, la media).        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para RandomForestRegressor:\n",
    "\n",
    "| Hiperparámetro      | Descripción |\n",
    "|---------------------|-------------|\n",
    "| `n_estimators`     | Número de árboles en el bosque. Más árboles pueden mejorar la precisión pero aumentan el tiempo de cómputo. |\n",
    "| `max_depth`        | Profundidad máxima de cada árbol. Un valor bajo puede llevar a *underfitting*, mientras que un valor alto puede llevar a *overfitting*. |\n",
    "| `max_features`     | Número de *features* consideradas en cada división. Puede ser un número entero, un porcentaje, `\"sqrt\"` o `\"log2\"`. Un menor número de *features* puede reducir la varianza (y con ello el *overfitting*). |\n",
    "| `min_samples_split` | Número mínimo de muestras necesarias para dividir un nodo. Valores más altos reducen el *overfitting*. |\n",
    "| `min_samples_leaf`  | Número mínimo de muestras en una hoja. Valores más altos suavizan la predicción. |\n",
    "| `max_samples`      | Porcentaje de muestras utilizadas en cada árbol. Útil para reducir *overfitting*. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajuste de hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1ª Iteración\n",
    "\n",
    "Vamos a empezar por una búsqueda randomizada preeliminar con un amplio espectro de valores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/NOCText4/Alejandro/miniconda3/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.6 s, sys: 4.54 s, total: 43.1 s\n",
      "Wall time: 59min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "param_dist = {\n",
    "    'preprocessing__geo__n_clusters': randint(low=3, high=200),\n",
    "    'random_forest__n_estimators': randint(100, 500),  # Cualquier entero entre 100 y 499\n",
    "    'random_forest__max_depth': randint(10, 110),      # Cualquier entero entre 10 y 109\n",
    "    'random_forest__min_samples_split': randint(2, 20),\n",
    "    'random_forest__min_samples_leaf': randint(1, 20),\n",
    "    'random_forest__max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "rnd_search = RandomizedSearchCV(\n",
    "    estimator = full_pipeline, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=40, \n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    random_state=42,\n",
    "    n_jobs=-1   # Usar todos los núcleos del CPU en paralelo\n",
    "    )\n",
    "\n",
    "_ = rnd_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```%%time``` es un [comando mágico de Jupyter](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-time) que mide el tiempo que tarda en ejecutarse la celda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver los resultados de los mejores modelos encontrados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_clusters</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>max_features</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>56</td>\n",
       "      <td>225</td>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>41574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>413</td>\n",
       "      <td>73</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>42508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>145</td>\n",
       "      <td>259</td>\n",
       "      <td>52</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>43156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>165</td>\n",
       "      <td>330</td>\n",
       "      <td>44</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>43172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>103</td>\n",
       "      <td>104</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>log2</td>\n",
       "      <td>43476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_clusters  n_estimators  max_depth  min_samples_split  min_samples_leaf  \\\n",
       "34          56           225         96                  2                 1   \n",
       "3          132           413         73                 13                 1   \n",
       "25         145           259         52                 14                 4   \n",
       "17         165           330         44                 11                 5   \n",
       "21         103           104         56                  2                 3   \n",
       "\n",
       "   max_features  mean_test_score  \n",
       "34         sqrt            41574  \n",
       "3          sqrt            42508  \n",
       "25         sqrt            43156  \n",
       "17         sqrt            43172  \n",
       "21         log2            43476  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_res = pd.DataFrame(rnd_search.cv_results_)\n",
    "cv_res.sort_values(by=\"mean_test_score\", ascending=False, inplace=True)\n",
    "\n",
    "cv_res = cv_res[['param_preprocessing__geo__n_clusters',\n",
    "                 'param_random_forest__n_estimators',\n",
    "                 'param_random_forest__max_depth',\n",
    "                 'param_random_forest__min_samples_split',\n",
    "                 'param_random_forest__min_samples_leaf',\n",
    "                 'param_random_forest__max_features',\n",
    "                 \"mean_test_score\"]]\n",
    "cv_res.columns = [\"n_clusters\", \"n_estimators\", \"max_depth\", \"min_samples_split\", \"min_samples_leaf\", \"max_features\", \"mean_test_score\"]\n",
    "\n",
    "cv_res[\"mean_test_score\"] = -cv_res[\"mean_test_score\"].round().astype(np.int64)\n",
    "cv_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos ir haciendo sucesivas pasadas fijando aquellas *features* donde todos los mejores resultados han convergido a un valor, y definiendo el diccionario de valores de prueba más cerrado sobre los mejores resultados. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2ª Iteración\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/NOCText4/Alejandro/miniconda3/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34 s, sys: 793 ms, total: 34.8 s\n",
      "Wall time: 10min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "full_pipeline.set_params(random_forest__max_features=\"sqrt\") # Fijamos el valor de max_features, que ha convergido a \"sqrt\"\n",
    "\n",
    "param_dist = {\n",
    "    'preprocessing__geo__n_clusters': randint(low=55, high=150),\n",
    "    'random_forest__n_estimators': randint(200, 300),\n",
    "    'random_forest__max_depth': randint(44, 97),\n",
    "    'random_forest__min_samples_split': randint(2, 14),\n",
    "    'random_forest__min_samples_leaf': randint(1, 5),\n",
    "}\n",
    "\n",
    "rnd_search = RandomizedSearchCV(\n",
    "    estimator = full_pipeline, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=40, \n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    random_state=42,\n",
    "    n_jobs=-1   # Usar todos los núcleos del CPU en paralelo\n",
    "    )\n",
    "\n",
    "_ = rnd_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_clusters</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>76</td>\n",
       "      <td>290</td>\n",
       "      <td>87</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>41604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>89</td>\n",
       "      <td>249</td>\n",
       "      <td>57</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>41678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>116</td>\n",
       "      <td>243</td>\n",
       "      <td>67</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>41747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>118</td>\n",
       "      <td>206</td>\n",
       "      <td>46</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>41846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>134</td>\n",
       "      <td>254</td>\n",
       "      <td>58</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>41848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_clusters  n_estimators  max_depth  min_samples_split  min_samples_leaf  \\\n",
       "6           76           290         87                  2                 1   \n",
       "14          89           249         57                  5                 1   \n",
       "27         116           243         67                  4                 1   \n",
       "9          118           206         46                  4                 1   \n",
       "8          134           254         58                  4                 2   \n",
       "\n",
       "    mean_test_score  \n",
       "6             41604  \n",
       "14            41678  \n",
       "27            41747  \n",
       "9             41846  \n",
       "8             41848  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_res = pd.DataFrame(rnd_search.cv_results_)\n",
    "cv_res.sort_values(by=\"mean_test_score\", ascending=False, inplace=True)\n",
    "\n",
    "cv_res = cv_res[['param_preprocessing__geo__n_clusters',\n",
    "                 'param_random_forest__n_estimators',\n",
    "                 'param_random_forest__max_depth',\n",
    "                 'param_random_forest__min_samples_split',\n",
    "                 'param_random_forest__min_samples_leaf',\n",
    "                 \"mean_test_score\"]]\n",
    "cv_res.columns = [\"n_clusters\", \"n_estimators\", \"max_depth\", \"min_samples_split\", \"min_samples_leaf\", \"mean_test_score\"]\n",
    "\n",
    "cv_res[\"mean_test_score\"] = -cv_res[\"mean_test_score\"].round().astype(np.int64)\n",
    "cv_res.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
