{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de predicción de esperanza de vida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construyendo el *pipeline* (objeto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def engineer_features(X):\n",
    "    X = X.copy()\n",
    "    # 1. Combine thinness features\n",
    "    X['thinness'] = X[['thinness1-19', 'thinness5-9']].mean(axis=1)\n",
    "    X.drop(columns=['thinness1-19', 'thinness5-9'], inplace=True)\n",
    "    # 2. Handle Income zeros\n",
    "    X['Income'] = X['Income'].replace(0, np.nan)\n",
    "    return X\n",
    "\n",
    "# Create custom transformer from function\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "feature_engineer_transformer = FunctionTransformer(engineer_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FuntionTransformer es una clase de scikit-learn que nos permite crear transformadores a partir de funciones que definamos. \n",
    "\n",
    "Otra forma de hacerlo sería definiendo directamente el transformador personalizado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class FeatureEngineerTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._feature_names_in = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Store feature names using scikit-learn's built-in validation\n",
    "        self._check_feature_names(X, reset=True)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        \n",
    "        # 1. Combine thinness features\n",
    "        X['thinness'] = X[['thinness1-19', 'thinness5-9']].mean(axis=1)\n",
    "        X.drop(columns=['thinness1-19', 'thinness5-9'], inplace=True)\n",
    "        # 2. Handle Income zeros\n",
    "        X['Income'] = X['Income'].replace(0, np.nan)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "feature_engineer_transformer = FeatureEngineerTransformer() # Create instance (object) of custom transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos decidido utilizar KNNImputer. Podríamos simplemente añadirlo al pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = Pipeline([\n",
    "    # Apply custom feature engineering\n",
    "    ('feature_engineering', feature_engineer_transformer),\n",
    "    \n",
    "    # First handle known good imputations\n",
    "    ('initial_imputer', ColumnTransformer([\n",
    "        ('median_imputer', SimpleImputer(strategy='median'), ['Schooling', 'Income', 'Total expenditure'])\n",
    "    ], remainder='passthrough')),\n",
    "    \n",
    "    # Then scale all features (required for KNN imputation)\n",
    "    ('scaler', StandardScaler()),\n",
    "    \n",
    "    # Finally handle remaining missing values with KNN on scaled data\n",
    "    ('knn_imputer', KNNImputer(n_neighbors=5))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, como hemos decidido calcular el número k de vecinos más cercanos de forma dinámica, necesitamos crear un transformador personalizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicKNNImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.knn = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Calculate k based on number of samples in training data\n",
    "        k = int(np.sqrt(X.shape[0]))\n",
    "        self.knn = KNNImputer(n_neighbors=k)\n",
    "        self.knn.fit(X)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.knn.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos ahora el pipeline completo de preprocesamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Pipeline([\n",
    "    # Apply custom feature engineering\n",
    "    ('feature_engineering', feature_engineer_transformer),\n",
    "    \n",
    "    # First handle known good imputations\n",
    "    ('initial_imputer', ColumnTransformer([\n",
    "        ('median_imputer', SimpleImputer(strategy='median'), ['Schooling', 'Income', 'Total expenditure'])\n",
    "    ], remainder='passthrough')),\n",
    "    \n",
    "    # Then scale all features (required for KNN imputation)\n",
    "    ('scaler', StandardScaler()),\n",
    "    \n",
    "    # Finally handle remaining missing values with KNN on scaled data\n",
    "    ('knn_imputer', DynamicKNNImputer())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y lo integramos con dos modelos para comparar su rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "pipeline1 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "pipeline2 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Pipeline* de entrenamiento (proceso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga y preparación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/life_expectancy.csv\")\n",
    "df.dropna(subset=['LifeExpectancy'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalmente eliminaremos las columnas con las que no trabajaremos: 'Country', 'Year' y 'Status' porque no las consideramos parte del modelo. Sin embargo, en este caso mantendremos 'Country' en una variable para poder separar los grupos de entrenamiento, validación y test garantizando que no se mezclen datos de un mismo país en diferentes conjuntos, evitando así la fuga de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_used_features = ['Country', 'Year', 'Status',\n",
    "            'InfantDeaths', # highly correlated with 'infantDeaths'\n",
    "            'Population', # low correlation and lots of missing values\n",
    "            'HepatitisB', # low correlation and lots of missing values\n",
    "            'Measles', # odd distribution and low correlation\n",
    "]\n",
    "\n",
    "groups = df['Country']  # guardada para separación de conjuntos\n",
    "df.drop(columns=[\"Country\", \"Year\", \"Status\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separación de conjuntos de entrenamiento y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['LifeExpectancy'])  # Features (exclude target)\n",
    "y = df['LifeExpectancy']                # Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_idx, test_idx in gss.split(X, y, groups):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elección de modelo mediante *cross-validation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos una validación cruzada usando como *fold* la separación por países."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression CV Results:\n",
      "  Mean MAE: 2.82 ± 0.33\n",
      "\n",
      "Random Forest CV Results:\n",
      "  Mean MAE: 2.07 ± 0.14\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, GroupKFold\n",
    "\n",
    "# Create group-aware cross-validation\n",
    "group_kfold = GroupKFold(n_splits=5)\n",
    "\n",
    "# Get groups for training set only\n",
    "groups_train = groups.iloc[train_idx]\n",
    "\n",
    "# Cross-validate both models\n",
    "scores1 = cross_val_score(\n",
    "    pipeline1,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=group_kfold,\n",
    "    groups=groups_train,\n",
    "    scoring='neg_mean_absolute_error'\n",
    ")\n",
    "\n",
    "scores2 = cross_val_score(\n",
    "    pipeline2,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=group_kfold,\n",
    "    groups=groups_train,\n",
    "    scoring='neg_mean_absolute_error'\n",
    ")\n",
    "\n",
    "# Convert to positive MAE values\n",
    "mae1 = -scores1\n",
    "mae2 = -scores2\n",
    "\n",
    "# Print cross-validation results\n",
    "print(\"Linear Regression CV Results:\")\n",
    "print(f\"  Mean MAE: {mae1.mean():.2f} ± {mae1.std():.2f}\")\n",
    "print(\"\\nRandom Forest CV Results:\")\n",
    "print(f\"  Mean MAE: {mae2.mean():.2f} ± {mae2.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación final del modelo elegido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, tras la evaluación de los modelos, seleccionamos el mejor y lo entrenamos con todos los datos de entrenamiento para obtener el modelo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Results:\n",
      "Best Model: Random Forest\n",
      "Test MAE: 2.12\n"
     ]
    }
   ],
   "source": [
    "# After cross-validation, select best model\n",
    "best_model = pipeline1 if mae1.mean() < mae2.mean() else pipeline2\n",
    "\n",
    "# Final evaluation on test set\n",
    "best_model.fit(X_train, y_train)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"\\nTest Set Results:\")\n",
    "print(f\"Best Model: {'Linear Regression' if best_model == pipeline1 else 'Random Forest'}\")\n",
    "print(f\"Test MAE: {test_mae:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardando el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../production/life_expectancy.joblib']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(best_model, \"../production/life_expectancy.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
