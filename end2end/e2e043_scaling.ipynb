{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento: escalamiento de *features*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook es una adaptación del [original de *Aurélien Gerón*](https://github.com/ageron/handson-ml3/blob/main/02_end_to_end_machine_learning_project.ipynb), de su libro: [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition. Aurélien Géron](https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pasos previos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "housing = pd.read_csv(\"./data/housing.csv\") \n",
    "\n",
    "# Generación de conjuntos de entrenamiento y prueba mediante muestreo estratificado por ingreso medio\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2,\n",
    "    stratify=pd.cut(housing[\"median_income\"], bins=[0., 1.5, 3.0, 4.5, 6., np.inf], labels=[1, 2, 3, 4, 5]),\n",
    "    random_state=42\n",
    "    )\n",
    "\n",
    "housing = train_set.drop(\"median_house_value\", axis=1) # Eliminamos la columna de la variable dependiente\n",
    "housing_labels = train_set[\"median_house_value\"].copy() # Guardamos la variable dependiente (etiquetas)\n",
    "\n",
    "housing_num = housing.select_dtypes(include=[np.number]) # seleccionamos las columnas numéricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escalado, normalización y estandarización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mayoría de los algoritmos de *Machine Learning* no funcionan bien cuando las *features* tienen escalas muy diferentes. Por ejemplo, muchos **clasificadores** calculan la distancia entre dos puntos mediante la distancia euclidiana. Si una de las características tiene valores mucho más grandes que las demás, la distancia se verá dominada por esta característica. Por ejemplo, en nuestro *dataset*, el rango de 'median_income' va de 0 a 15, mientras que el rango de 'total_rooms' va de 6 a 39.320.\n",
    "\n",
    "Para evitarlo, es habitual escalar las *features*.\n",
    "\n",
    "La terminología puede ser confusa en este punto. En general, la **normalización** se refiere a cambiar la escala de los datos para que se ajusten a un rango específico, mientras que la **estandarización** se refiere a cambiar la distribución de los datos para que tengan una media de 0 y una desviación estándar de 1. En ambos casos, se trata de transformaciones lineales que no cambian la forma de la distribución de los datos. En estadística suele haber una distinción clara entre ambos términos, pero en el aprendizaje profundo y en la visión por computadora, la terminología puede ser menos consistente y es habitual que se use \"normalización\" para referirse a la estandarización.\n",
    "\n",
    "<!-- TODO: https://en.wikipedia.org/wiki/Normalization_(machine_learning) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>longitude</th>\n",
       "      <td>16512.0</td>\n",
       "      <td>-119.575635</td>\n",
       "      <td>2.001828</td>\n",
       "      <td>-124.3500</td>\n",
       "      <td>-121.80000</td>\n",
       "      <td>-118.51000</td>\n",
       "      <td>-118.010000</td>\n",
       "      <td>-114.3100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latitude</th>\n",
       "      <td>16512.0</td>\n",
       "      <td>35.639314</td>\n",
       "      <td>2.137963</td>\n",
       "      <td>32.5400</td>\n",
       "      <td>33.94000</td>\n",
       "      <td>34.26000</td>\n",
       "      <td>37.720000</td>\n",
       "      <td>41.9500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>housing_median_age</th>\n",
       "      <td>16512.0</td>\n",
       "      <td>28.653404</td>\n",
       "      <td>12.574819</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>18.00000</td>\n",
       "      <td>29.00000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>52.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_rooms</th>\n",
       "      <td>16512.0</td>\n",
       "      <td>2622.539789</td>\n",
       "      <td>2138.417080</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>1443.00000</td>\n",
       "      <td>2119.00000</td>\n",
       "      <td>3141.000000</td>\n",
       "      <td>39320.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_bedrooms</th>\n",
       "      <td>16354.0</td>\n",
       "      <td>534.914639</td>\n",
       "      <td>412.665649</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>295.00000</td>\n",
       "      <td>433.00000</td>\n",
       "      <td>644.000000</td>\n",
       "      <td>6210.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>population</th>\n",
       "      <td>16512.0</td>\n",
       "      <td>1419.687379</td>\n",
       "      <td>1115.663036</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>784.00000</td>\n",
       "      <td>1164.00000</td>\n",
       "      <td>1719.000000</td>\n",
       "      <td>35682.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>households</th>\n",
       "      <td>16512.0</td>\n",
       "      <td>497.011810</td>\n",
       "      <td>375.696156</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>279.00000</td>\n",
       "      <td>408.00000</td>\n",
       "      <td>602.000000</td>\n",
       "      <td>5358.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median_income</th>\n",
       "      <td>16512.0</td>\n",
       "      <td>3.875884</td>\n",
       "      <td>1.904931</td>\n",
       "      <td>0.4999</td>\n",
       "      <td>2.56695</td>\n",
       "      <td>3.54155</td>\n",
       "      <td>4.745325</td>\n",
       "      <td>15.0001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      count         mean          std       min         25%  \\\n",
       "longitude           16512.0  -119.575635     2.001828 -124.3500  -121.80000   \n",
       "latitude            16512.0    35.639314     2.137963   32.5400    33.94000   \n",
       "housing_median_age  16512.0    28.653404    12.574819    1.0000    18.00000   \n",
       "total_rooms         16512.0  2622.539789  2138.417080    6.0000  1443.00000   \n",
       "total_bedrooms      16354.0   534.914639   412.665649    2.0000   295.00000   \n",
       "population          16512.0  1419.687379  1115.663036    3.0000   784.00000   \n",
       "households          16512.0   497.011810   375.696156    2.0000   279.00000   \n",
       "median_income       16512.0     3.875884     1.904931    0.4999     2.56695   \n",
       "\n",
       "                           50%          75%         max  \n",
       "longitude           -118.51000  -118.010000   -114.3100  \n",
       "latitude              34.26000    37.720000     41.9500  \n",
       "housing_median_age    29.00000    37.000000     52.0000  \n",
       "total_rooms         2119.00000  3141.000000  39320.0000  \n",
       "total_bedrooms       433.00000   644.000000   6210.0000  \n",
       "population          1164.00000  1719.000000  35682.0000  \n",
       "households           408.00000   602.000000   5358.0000  \n",
       "median_income          3.54155     4.745325     15.0001  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La normalización más común es la **normalización min-max** o ***min-max scaling***. La **normalización min-max** es la más sencilla: los valores se escalan y desplazan de forma que terminen en el rango entre un valor mínimo y un valor máximo. Normalmente será entre 0 y 1, aunque pueden ser otros (las redes neuronales suelen funcionar mejor con *inputs* con media 0, por lo que a veces se usa el rango -1 a 1). Scikit-Learn proporciona una clase `MinMaxScaler` para esto.\n",
    "\n",
    "$$ X_{norm} = \\frac{X - X_{min}}{X_{max} - X_{min}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La normalización Min-Max es muy sensible a los *outliers*, ya que único valor muy grande puede cambiar completamente la escala de los datos. En una situación donde todos los datos están entre 20 y 30 pero aparece un único valor de 100, el máximo pasa a 100, desplazando todos los demás valores a un rango muy bajo. En general, la normalización Min-Max se debe usar solo si estamos seguros de que los *outliers* no son errores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por su parte, la **estandarización Z-score** (***standard score***) es diferente: primero resta la media (para que ésta sea 0), y luego divide por la **desviación típica (*standard deviation*)** para que la distribución resultante tenga desviación estándar 1. A diferencia del escalado min-max, la estandarización no limita los valores entre un rango concreto, pero esto tiene también la ventaja de ser mucho menos sensible a los valores atípicos. Scikit-Learn proporciona una clase `StandardScaler` para esto.\n",
    "\n",
    "$$ X_{std} = \\frac{X - \\mu}{\\sigma} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "housing_num_std_scaled = std_scaler.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muchos modelos de ML funcionan mejor estandarizando las *features* de entrada y es una práctica habitual y sistemática en la mayoría de los casos (salvo para modelos basados en árboles). Escalar el *target* es menos común, pero puede ser útil en algunos casos, particularmente para modelos basados en gradientes (como las redes neuronales) o modelos basados en distancias (como regresiones KNN o SVM).\n",
    "\n",
    "Por ejemplo, podríamos aplicar de nuevo StandardScarler a las etiquetas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.16601465],\n",
       "       [ 0.62745084],\n",
       "       [-1.07439665],\n",
       "       ...,\n",
       "       [-0.5756836 ],\n",
       "       [ 0.44162188],\n",
       "       [-1.2472608 ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_scaler = StandardScaler()\n",
    "scaled_labels = target_scaler.fit_transform(housing_labels.to_frame()) # convertimos el target a un dataframe (fit_transform espera 2D)\n",
    "print(type(housing_labels)) # Al ser una única columna, las etiquetas fueron almacenadas antes en un objeto Series\n",
    "scaled_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escalado de variables objetivo y posterior inversión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si transformamos de cualquier manera la variable objetivo, la salida de nuestro modelo también devolverá predicciones transformadas. Si queremos que las predicciones estén en la escala original, necesitaremos invertir la transformación. Muchos de los transformadores de Scikit-Learn tienen un método `inverse_transform()`, lo que facilita calcular la inversa de sus transformaciones.\n",
    "<!-- TODO: Los transformadores categóricos no lo tienen ya que entonces estaríamos en un problema de clasificación. // No todos son invertibles? -->\n",
    "\n",
    "Por poner un ejemplo simplificado, vamos a entrenar una **regresión lineal simple con el predictor que más correla** (median_income) y las etiquetas que acabamos de escalar. Seguidamente vamos a probar sus predicciones con el conjunto de test y deshacer la transformación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[135958.75805364],\n",
       "       [309735.008975  ],\n",
       "       [165232.3998617 ],\n",
       "       [138162.41971241],\n",
       "       [232903.1766333 ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(housing[[\"median_income\"]], scaled_labels) # entrenamos el modelo con las variables independientes escaladas\n",
    "\n",
    "some_new_data = housing[[\"median_income\"]].iloc[:5]  # por simplicidad, simulamos nuevos inputs para predecir tomando 5 filas (no hemos preprocesado el conjunto de test)\n",
    "\n",
    "scaled_predictions = model.predict(some_new_data)\n",
    "target_scaler.inverse_transform(scaled_predictions) # Deshacemos la transformación para obtener las predicciones en la escala original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este proceso se puede simplificar utilizando la clase `TransformedTargetRegressor` de Scikit-Learn, que permite entrenar un modelo con las etiquetas transformadas y deshacer la transformación automáticamente.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([135958.75805364, 309735.008975  , 165232.3998617 , 138162.41971241,\n",
       "       232903.1766333 ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "model = TransformedTargetRegressor(regressor = LinearRegression(),\n",
    "                                   transformer = StandardScaler()) # transformador de la variable dependiente\n",
    "model.fit(housing[[\"median_income\"]], housing_labels)\n",
    "model.predict(some_new_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
