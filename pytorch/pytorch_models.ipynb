{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db412e85",
   "metadata": {},
   "source": [
    "# Construyendo modelos con PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2c2e0e",
   "metadata": {},
   "source": [
    "## Modelos básicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.nn.Module` y `torch.nn.Parameter`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`torch.nn.Module` es la clase base para cualquier red neuronal en PyTorch. Cualquier clase que herede de `torch.nn.Module` debe implementar el método `forward`. El método `forward` es el que define cómo se calcula la salida de la red neuronal.\n",
    "\n",
    "Cualquier objeto de tipo `torch.nn.Module` registra todos los parámetros de la red neuronal (los pesos y los sesgos). Estos parámetros son objetos de tipo `torch.nn.Parameter`, que es una subclase de `torch.Tensor`. Los parámetros se pueden acceder a través del método `parameters()` de la clase `Module`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestra un ejemplo de cómo definir una red neuronal muy básica en PyTorch. Esta red consta de dos capas lineales y una función de activación ReLU entre ellas. En ella podemos ver la estructura básica de una red neuronal en PyTorch, con un método `__init__()` que define las capas y otros componentes de la red, y un método `forward()` donde se realiza la computación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model:\n",
      "TinyModel(\n",
      "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n",
      "\n",
      "\n",
      "Just one layer:\n",
      "Linear(in_features=200, out_features=10, bias=True)\n",
      "\n",
      "\n",
      "Model params:\n",
      "Parameter containing:\n",
      "tensor([[-0.0165, -0.0320, -0.0229,  ..., -0.0139, -0.0368,  0.0666],\n",
      "        [-0.0647,  0.0893, -0.0493,  ...,  0.0397, -0.0546, -0.0781],\n",
      "        [-0.0740, -0.0500, -0.0597,  ...,  0.0890, -0.0705, -0.0604],\n",
      "        ...,\n",
      "        [-0.0829, -0.0146,  0.0345,  ..., -0.0057,  0.0829,  0.0737],\n",
      "        [-0.0223, -0.0434, -0.0292,  ..., -0.0340,  0.0451,  0.0523],\n",
      "        [-0.0747, -0.0579,  0.0683,  ...,  0.0150, -0.0853,  0.0460]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0587, -0.0109,  0.0101, -0.0069,  0.0317,  0.0026,  0.0690, -0.0775,\n",
      "        -0.0985,  0.0605, -0.0362,  0.0442,  0.0301, -0.0218, -0.0363,  0.0097,\n",
      "         0.0359,  0.0279, -0.0797, -0.0368, -0.0204, -0.0146, -0.0700,  0.0896,\n",
      "         0.0429, -0.0466,  0.0119, -0.0401,  0.0614,  0.0954,  0.0135,  0.0598,\n",
      "        -0.0901, -0.0858,  0.0289,  0.0391,  0.0965, -0.0035,  0.0616,  0.0662,\n",
      "        -0.0113,  0.0824,  0.0218, -0.0448, -0.0395, -0.0603, -0.0374,  0.0433,\n",
      "         0.0313,  0.0902, -0.0963, -0.0342,  0.0986,  0.0246, -0.0249, -0.0651,\n",
      "         0.0778, -0.0705, -0.0454,  0.0535,  0.0802, -0.0550, -0.0826, -0.0011,\n",
      "        -0.0772, -0.0826, -0.0865,  0.0194,  0.0561, -0.0706,  0.0364,  0.0487,\n",
      "        -0.0305,  0.0986, -0.0772, -0.0327,  0.0845, -0.0090, -0.0836,  0.0278,\n",
      "         0.0520, -0.0221,  0.0371,  0.0015,  0.0162,  0.0931, -0.0587,  0.0759,\n",
      "         0.0545, -0.0988, -0.0414, -0.0564, -0.0349, -0.0190, -0.0561,  0.0395,\n",
      "        -0.0936, -0.0932,  0.0478,  0.0496,  0.0685,  0.0468, -0.0572, -0.0800,\n",
      "        -0.0203, -0.0652,  0.0460, -0.0948,  0.0881,  0.0456,  0.0104, -0.0890,\n",
      "         0.0765,  0.0384, -0.0707,  0.0017,  0.0846,  0.0050, -0.0111,  0.0554,\n",
      "        -0.0215, -0.0934,  0.0366,  0.0251, -0.0594, -0.0612,  0.0555,  0.0741,\n",
      "         0.0364, -0.0553,  0.0802,  0.0208, -0.0433,  0.0877, -0.0151,  0.0035,\n",
      "         0.0476, -0.0641, -0.0110, -0.0706,  0.0154,  0.0523,  0.0320, -0.0394,\n",
      "         0.0451, -0.0391, -0.0388, -0.0867, -0.0578,  0.0176,  0.0352, -0.0849,\n",
      "        -0.0267,  0.0194, -0.0600, -0.0401, -0.0252,  0.0970,  0.0120,  0.0277,\n",
      "        -0.0493,  0.0200, -0.0353,  0.0911,  0.0677, -0.0935, -0.0642,  0.0386,\n",
      "        -0.0899, -0.0097,  0.0922,  0.0592,  0.0888,  0.0107,  0.0820, -0.0833,\n",
      "        -0.0807, -0.0879,  0.0138,  0.0347,  0.0153, -0.0914, -0.0912,  0.0708,\n",
      "        -0.0401,  0.0598, -0.0303,  0.0937, -0.0943, -0.0149,  0.0298,  0.0220,\n",
      "         0.0247, -0.0959,  0.0681,  0.0505, -0.0848, -0.0641,  0.0820, -0.0942],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0040,  0.0597,  0.0378,  ...,  0.0153, -0.0004,  0.0695],\n",
      "        [ 0.0010, -0.0103, -0.0546,  ..., -0.0295,  0.0518, -0.0104],\n",
      "        [ 0.0472,  0.0428, -0.0081,  ...,  0.0586,  0.0193, -0.0179],\n",
      "        ...,\n",
      "        [-0.0034, -0.0559,  0.0353,  ...,  0.0464, -0.0445,  0.0047],\n",
      "        [ 0.0684,  0.0276, -0.0268,  ...,  0.0538,  0.0426,  0.0547],\n",
      "        [-0.0414, -0.0067,  0.0659,  ...,  0.0375, -0.0407, -0.0072]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0330,  0.0631,  0.0446,  0.0035, -0.0126,  0.0349, -0.0547, -0.0270,\n",
      "         0.0034,  0.0137], requires_grad=True)\n",
      "\n",
      "\n",
      "Layer params:\n",
      "Parameter containing:\n",
      "tensor([[ 0.0040,  0.0597,  0.0378,  ...,  0.0153, -0.0004,  0.0695],\n",
      "        [ 0.0010, -0.0103, -0.0546,  ..., -0.0295,  0.0518, -0.0104],\n",
      "        [ 0.0472,  0.0428, -0.0081,  ...,  0.0586,  0.0193, -0.0179],\n",
      "        ...,\n",
      "        [-0.0034, -0.0559,  0.0353,  ...,  0.0464, -0.0445,  0.0047],\n",
      "        [ 0.0684,  0.0276, -0.0268,  ...,  0.0538,  0.0426,  0.0547],\n",
      "        [-0.0414, -0.0067,  0.0659,  ...,  0.0375, -0.0407, -0.0072]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0330,  0.0631,  0.0446,  0.0035, -0.0126,  0.0349, -0.0547, -0.0270,\n",
      "         0.0034,  0.0137], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self): # Definimos las capas como atributos \n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(100, 200) # Capa de entrada\n",
    "        self.activation = torch.nn.ReLU() # Función de activación\n",
    "        self.linear2 = torch.nn.Linear(200, 10) # Capa de salida\n",
    "        self.softmax = torch.nn.Softmax() # Función de salida\n",
    "    \n",
    "    def forward(self, x): # Definimos el flujo de datos\n",
    "        x = self.linear1(x) # Capa de entrada\n",
    "        x = self.activation(x) # Función de activación\n",
    "        x = self.linear2(x) # Capa de salida\n",
    "        x = self.softmax(x) # Función de salida\n",
    "        return x\n",
    "\n",
    "tinymodel = TinyModel()\n",
    "\n",
    "print('The model:')\n",
    "print(tinymodel)\n",
    "\n",
    "print('\\n\\nJust one layer:')\n",
    "print(tinymodel.linear2)\n",
    "\n",
    "print('\\n\\nModel params:')\n",
    "for param in tinymodel.parameters():\n",
    "    print(param)\n",
    "\n",
    "print('\\n\\nLayer params:')\n",
    "for param in tinymodel.linear2.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estilo funcional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librería [`torch.nn.functional`](https://pytorch.org/docs/stable/nn.functional.html) permite llamar a algunos de los elementos (típicamente funciones de activación) directamente como funciones en lugar de cómo atributos de un objeto. Por ejemplo, el modelo anterior es equivalente al siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F \n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(100, 200)\n",
    "        self.linear2 = torch.nn.Linear(200, 10) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x) \n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = F.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    # o también es equivalente a:\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.softmax(self.linear2(x))\n",
    "        return x\n",
    "    \n",
    "    # o aún más compacto:\n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.linear2(F.relu(self.linear1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando `Secuential`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Sequential` es una clase que permite definir una red neuronal secuencialmente. Es decir, se pueden definir las capas de la red neuronal en el orden en el que se van a aplicar. A continuación se muestra cómo se puede definir el modelo anterior usando `nn.Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.model = nn.Sequential( # Definimos las capas en orden como un único atributo \n",
    "        nn.Linear(100, 200),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(200, 10),\n",
    "        nn.Softmax()\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.model(x) # Se llama a toda la secuencia, su orden ya está definido internamente\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capas lineales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tipo más básico de capa de red neuronal es una capa *lineal* o *totalmente conectada*. Esta es una capa en la que cada entrada influye en cada salida de la capa en un grado especificado por los pesos de la capa. Si un modelo tiene *m* entradas y *n* salidas, los pesos serán una matriz *m* x *n*.\n",
    "\n",
    "Se llama *lineal* porque la salida de la capa es una combinación lineal de las entradas $y=Wx+b$, donde $W$ es la matriz de pesos, $x$ es el vector de entradas y $b$ es el vector de sesgos.\n",
    "\n",
    "Si tenemos 3 entradas $x_1$, $x_2$ y $x_3$ y 2 salidas $y_1$ y $y_2$, la salida de la capa será:\n",
    "\n",
    "$$\\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix} = \\begin{bmatrix} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros de la capa (pesos y sesgos):\n",
      "Parameter containing:\n",
      "tensor([[-0.2069,  0.2911, -0.0554],\n",
      "        [-0.3530, -0.0932,  0.4839]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.5717, -0.4867], requires_grad=True)\n",
      "\n",
      "\n",
      "Pesos: Parameter containing:\n",
      "tensor([[-0.2069,  0.2911, -0.0554],\n",
      "        [-0.3530, -0.0932,  0.4839]], requires_grad=True)\n",
      "\n",
      "\n",
      "Sesgos: Parameter containing:\n",
      "tensor([-0.5717, -0.4867], requires_grad=True)\n",
      "\n",
      "\n",
      "Input: tensor([[0.4343, 0.9277, 0.8893]])\n",
      "\n",
      "\n",
      "Output:\n",
      "tensor([[-0.4408, -0.2962]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lin = torch.nn.Linear(3, 2) # 3 entradas, 2 salidas\n",
    "\n",
    "print('Parámetros de la capa (pesos y sesgos):')\n",
    "for param in lin.parameters():\n",
    "    print(param)\n",
    "\n",
    "# Lo mismo accediendo directamente a cada atributo:\n",
    "print('\\n\\nPesos:', lin.weight)\n",
    "print('\\n\\nSesgos:', lin.bias)\n",
    "\n",
    "x = torch.rand(1, 3) # Tensor de entrada de 1x3\n",
    "print('\\n\\nInput:', x)\n",
    "\n",
    "y = lin(x)\n",
    "print('\\n\\nOutput:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que `lin.weight` contiene la matriz de pesos y que `lin.bias` contiene el vector de sesgos, siendo ambos de tipo `Parameter`.\n",
    "\n",
    "`Parameter` es una subclase de `Tensor` que se utiliza para indicar que un tensor es un parámetro de una red neuronal y, por lo tanto, debe registrar los gradientes por el módulo de autograd de PyTorch. Esto es importante para que PyTorch pueda calcular los gradientes de los parámetros durante el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de activación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si todo lo que hiciéramos fuera multiplicar tensores por los pesos de las capas repetidamente, solo podríamos simular funciones lineales; además, no tendría sentido tener muchas capas, ya que toda la red se reduciría a una sola multiplicación de matrices. Insertar funciones de activación no lineales entre capas es lo que permite que un modelo de aprendizaje profundo simule cualquier función, en lugar de solo las lineales.\n",
    "\n",
    "Las funciones de activación más comunes son la función sigmoide, la tangente hiperbólica y la función **ReLU**. La función sigmoide y la tangente hiperbólica se utilizan a menudo en redes neuronales más antiguas, pero la función ReLU es la más común en la actualidad. La función ReLU es simplemente $f(x) = \\max(0, x)$, lo que significa que si la entrada es negativa, la salida es cero, y si la entrada es positiva, la salida es igual a la entrada.\n",
    "\n",
    "En clasificación binaria, la función de activación **sigmoide** es comúnmente utilizada en la capa de salida, ya que la salida de la función sigmoide está en el rango [0, 1], lo que es adecuado para representar probabilidades. En clasificación multiclase, la función de activación softmax es comúnmente utilizada en la capa de salida, ya que la salida de la función softmax es un vector de probabilidades que suman 1.\n",
    "\n",
    "En clasificación multiclase, la función de activación ***softmax*** es comúnmente utilizada en la capa de salida, que es una generalización de la función sigmoide para múltiples clases. La función softmax asigna a cada clase una probabilidad entre 0 y 1, de forma que la suma de las probabilidades de todas las clases es 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuentes\n",
    "\n",
    "- https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
