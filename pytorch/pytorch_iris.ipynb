{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de iris con red neuronal simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    " \n",
    "iris = load_iris()\n",
    "print(iris.data.dtype)\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    " \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos los arrays de NumPy a tensores de PyTorch.\n",
    "\n",
    "Scikit-learn utiliza a menudo floats de 64 bits para tener más precisión, pero PyTorch utiliza floats de 32 bits por defecto, priorizando la eficiencia al ser utilizado para datasets mas grandes normalmente más robustosa pequeñas variaciones en los datos.\n",
    "\n",
    "Por lo tanto, convertimos los arrays de numpy a tensores de PyTorch pasando el tipo de dato a `torch.float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos un modelo de red neuronal simple parametrizable con el número de entradas y salidas y el número de neuronas en la capa oculta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # Input layer\n",
    "        self.relu = nn.ReLU()                          # Activation function\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size) # Output layer\n",
    "         \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intanciamos la red neuronal con el número de entradas y salidas correspondientes a los datos de iris, y 10 neuronas en la capa oculta.\n",
    "\n",
    "Definimos el optimizador Adam y la función de pérdida de entropía cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42) # Fijamos la semilla para asegurar reproducibilidad\n",
    "\n",
    "input_size = iris.data.shape[1] # Capa de entrada: número de características\n",
    "hidden_size = 10 # Tamaño de la capa oculta\n",
    "output_size = len(iris.target_names) # Capa de salida: número de clases\n",
    "\n",
    "model = SimpleNN(input_size, hidden_size, output_size) # Instanciamos el modelo\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # Función de pérdida\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01) # Optimizador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos el entrenamiento del modelo. En este aso, al ser un dataset muy pequeño, pasamos todos los datos de entrenamiento en un solo ***batch***, por lo que no se realiza un entrenamiento por lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 0.7783\n",
      "Epoch [20/1000], Loss: 0.5399\n",
      "Epoch [30/1000], Loss: 0.3921\n",
      "Epoch [40/1000], Loss: 0.2934\n",
      "Epoch [50/1000], Loss: 0.2166\n",
      "Epoch [60/1000], Loss: 0.1639\n",
      "Epoch [70/1000], Loss: 0.1284\n",
      "Epoch [80/1000], Loss: 0.1050\n",
      "Epoch [90/1000], Loss: 0.0902\n",
      "Epoch [100/1000], Loss: 0.0800\n",
      "Epoch [110/1000], Loss: 0.0732\n",
      "Epoch [120/1000], Loss: 0.0683\n",
      "Epoch [130/1000], Loss: 0.0647\n",
      "Epoch [140/1000], Loss: 0.0619\n",
      "Epoch [150/1000], Loss: 0.0597\n",
      "Epoch [160/1000], Loss: 0.0579\n",
      "Epoch [170/1000], Loss: 0.0564\n",
      "Epoch [180/1000], Loss: 0.0551\n",
      "Epoch [190/1000], Loss: 0.0541\n",
      "Epoch [200/1000], Loss: 0.0532\n",
      "Epoch [210/1000], Loss: 0.0524\n",
      "Epoch [220/1000], Loss: 0.0517\n",
      "Epoch [230/1000], Loss: 0.0511\n",
      "Epoch [240/1000], Loss: 0.0506\n",
      "Epoch [250/1000], Loss: 0.0501\n",
      "Epoch [260/1000], Loss: 0.0498\n",
      "Epoch [270/1000], Loss: 0.0494\n",
      "Epoch [280/1000], Loss: 0.0491\n",
      "Epoch [290/1000], Loss: 0.0488\n",
      "Epoch [300/1000], Loss: 0.0486\n",
      "Epoch [310/1000], Loss: 0.0484\n",
      "Epoch [320/1000], Loss: 0.0482\n",
      "Epoch [330/1000], Loss: 0.0480\n",
      "Epoch [340/1000], Loss: 0.0479\n",
      "Epoch [350/1000], Loss: 0.0478\n",
      "Epoch [360/1000], Loss: 0.0477\n",
      "Epoch [370/1000], Loss: 0.0476\n",
      "Epoch [380/1000], Loss: 0.0475\n",
      "Epoch [390/1000], Loss: 0.0474\n",
      "Epoch [400/1000], Loss: 0.0474\n",
      "Epoch [410/1000], Loss: 0.0473\n",
      "Epoch [420/1000], Loss: 0.0473\n",
      "Epoch [430/1000], Loss: 0.0472\n",
      "Epoch [440/1000], Loss: 0.0472\n",
      "Epoch [450/1000], Loss: 0.0471\n",
      "Epoch [460/1000], Loss: 0.0471\n",
      "Epoch [470/1000], Loss: 0.0471\n",
      "Epoch [480/1000], Loss: 0.0470\n",
      "Epoch [490/1000], Loss: 0.0470\n",
      "Epoch [500/1000], Loss: 0.0470\n",
      "Epoch [510/1000], Loss: 0.0470\n",
      "Epoch [520/1000], Loss: 0.0470\n",
      "Epoch [530/1000], Loss: 0.0469\n",
      "Epoch [540/1000], Loss: 0.0469\n",
      "Epoch [550/1000], Loss: 0.0469\n",
      "Epoch [560/1000], Loss: 0.0469\n",
      "Epoch [570/1000], Loss: 0.0469\n",
      "Epoch [580/1000], Loss: 0.0469\n",
      "Epoch [590/1000], Loss: 0.0469\n",
      "Epoch [600/1000], Loss: 0.0468\n",
      "Epoch [610/1000], Loss: 0.0468\n",
      "Epoch [620/1000], Loss: 0.0468\n",
      "Epoch [630/1000], Loss: 0.0468\n",
      "Epoch [640/1000], Loss: 0.0468\n",
      "Epoch [650/1000], Loss: 0.0468\n",
      "Epoch [660/1000], Loss: 0.0468\n",
      "Epoch [670/1000], Loss: 0.0468\n",
      "Epoch [680/1000], Loss: 0.0468\n",
      "Epoch [690/1000], Loss: 0.0468\n",
      "Epoch [700/1000], Loss: 0.0468\n",
      "Epoch [710/1000], Loss: 0.0468\n",
      "Epoch [720/1000], Loss: 0.0468\n",
      "Epoch [730/1000], Loss: 0.0468\n",
      "Epoch [740/1000], Loss: 0.0467\n",
      "Epoch [750/1000], Loss: 0.0467\n",
      "Epoch [760/1000], Loss: 0.0467\n",
      "Epoch [770/1000], Loss: 0.0467\n",
      "Epoch [780/1000], Loss: 0.0467\n",
      "Epoch [790/1000], Loss: 0.0467\n",
      "Epoch [800/1000], Loss: 0.0467\n",
      "Epoch [810/1000], Loss: 0.0467\n",
      "Epoch [820/1000], Loss: 0.0467\n",
      "Epoch [830/1000], Loss: 0.0467\n",
      "Epoch [840/1000], Loss: 0.0467\n",
      "Epoch [850/1000], Loss: 0.0467\n",
      "Epoch [860/1000], Loss: 0.0467\n",
      "Epoch [870/1000], Loss: 0.0467\n",
      "Epoch [880/1000], Loss: 0.0467\n",
      "Epoch [890/1000], Loss: 0.0467\n",
      "Epoch [900/1000], Loss: 0.0467\n",
      "Epoch [910/1000], Loss: 0.0467\n",
      "Epoch [920/1000], Loss: 0.0467\n",
      "Epoch [930/1000], Loss: 0.0467\n",
      "Epoch [940/1000], Loss: 0.0467\n",
      "Epoch [950/1000], Loss: 0.0467\n",
      "Epoch [960/1000], Loss: 0.0467\n",
      "Epoch [970/1000], Loss: 0.0467\n",
      "Epoch [980/1000], Loss: 0.0467\n",
      "Epoch [990/1000], Loss: 0.0467\n",
      "Epoch [1000/1000], Loss: 0.0467\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(X_train_tensor) # Forward pass\n",
    "    loss = criterion(outputs, y_train_tensor) # Calcular la pérdida\n",
    "    \n",
    "    optimizer.zero_grad() # Inicializar los gradientes a cero\n",
    "    loss.backward() # Backward pass\n",
    "    optimizer.step() # Actualizar los parámetros\n",
    "    \n",
    "    if (epoch+1) % 10 == 0: # Cada 10 epochs muestra la pérdida\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos la exactitud del modelo con el conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 1.00\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\tX_test_tensor = torch.FloatTensor(X_test)\n",
    "\ty_test_tensor = torch.LongTensor(y_test)\n",
    "\toutputs = model(X_test_tensor)\n",
    "\t_, predicted = torch.max(outputs, 1)\n",
    "\taccuracy = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)\n",
    "\tprint(f'Accuracy on the test set: {accuracy:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
